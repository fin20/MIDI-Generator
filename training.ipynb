{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqHQ+BdrXnkm9w0VBrvBDp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1KvYKclXOGJ_","executionInfo":{"status":"ok","timestamp":1744367127653,"user_tz":-60,"elapsed":18083,"user":{"displayName":"Fin Galletly","userId":"15278410467358031190"}},"outputId":"fd6824c2-11e6-42ba-e9e7-b2c9cfa88571"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["import glob\n","import pickle\n","from unittest import result\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","from typing import cast\n","from music21 import converter, instrument, note, chord, stream\n","from music21.stream.base import Score\n","import tensorflow as tf\n","from pathlib import Path\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential, Model, load_model\n","from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, BatchNormalization, Input, concatenate\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"],"metadata":{"id":"iHqo82NAOLNh","executionInfo":{"status":"ok","timestamp":1744367138279,"user_tz":-60,"elapsed":9034,"user":{"displayName":"Fin Galletly","userId":"15278410467358031190"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def get_songs():\n","    songs = []\n","    folder = Path('/content/drive/MyDrive/finalyearproject/LakhMIDI')\n","    for file in folder.rglob('*.mid'):\n","        songs.append(file)\n","\n","    result = random.sample(songs, 100)\n","    # print(result)\n","    return result"],"metadata":{"id":"7r8F3fDGON7v","executionInfo":{"status":"ok","timestamp":1744375539248,"user_tz":-60,"elapsed":7,"user":{"displayName":"Fin Galletly","userId":"15278410467358031190"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def quantise_value(value, resolution=0.25):\n","    return round(value / resolution) * resolution\n"],"metadata":{"id":"ZHRLr3TlOs3n","executionInfo":{"status":"ok","timestamp":1744367143143,"user_tz":-60,"elapsed":2,"user":{"displayName":"Fin Galletly","userId":"15278410467358031190"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def get_notes():\n","    notes_file = Path('/content/drive/MyDrive/finalyearproject/notes/notes7.pkl')\n","\n","    songs = get_songs()\n","    notes = []\n","\n","    # load existing notes\n","    if notes_file.exists():\n","        with open(notes_file, 'rb') as f:\n","            notes = pickle.load(f)\n","        print(f\" Resuming from {len(notes)} existing notes\")\n","\n","    batch_notes = []\n","\n","    # process each song and extract note/chord data\n","    for i, song in enumerate(tqdm(songs, desc=\"Processing songs\")):\n","        try:\n","            midi = converter.parse(song)\n","            parsing_notes = None\n","\n","            try:\n","                ins = instrument.partitionByInstrument(midi)\n","                if isinstance(ins, Score):\n","                    parsing_notes = ins.parts[0].recurse()\n","                else:\n","                    parsing_notes = midi.flatten().notes\n","            except Exception as e:\n","                print(f\" Error processing {song}: {e}\")\n","                continue\n","\n","            offsetBase = 0\n","\n","            # go through each note/chord and format its represenation\n","            if parsing_notes is not None:\n","                for element in parsing_notes:\n","                    if isinstance(element, note.Note):\n","                        q_duration = quantise_value(element.duration.quarterLength, resolution=0.25)\n","                        q_offset = quantise_value(element.offset - offsetBase, resolution=0.25)\n","                        note_str = f\"{element.pitch}:{q_duration}:{q_offset}\" # pitch:duration:offset\n","                        batch_notes.append(note_str)\n","                        offsetBase = element.offset\n","                    elif isinstance(element, chord.Chord):\n","                        chord_components = '.'.join(str(n) for n in element.normalOrder)\n","                        q_duration = quantise_value(element.duration.quarterLength, resolution=0.25)\n","                        q_offset = quantise_value(element.offset - offsetBase, resolution=0.25)\n","                        chord_str = f\"{chord_components}:{q_duration}:{q_offset}\" # chord:duration:offset\n","                        batch_notes.append(chord_str)\n","                        offsetBase = element.offset\n","\n","        except Exception as e:\n","            print(f\" Skipping {song} due to error: {e}\")\n","            continue\n","\n","        # save and clear batch every 50 songs\n","        if (i + 1) % 50 == 0:\n","            notes.extend(batch_notes)  # add batch to full list\n","            with open(notes_file, 'wb') as f:\n","                pickle.dump(notes, f)  # save full list to file\n","            print(f\" Checkpoint: Saved {len(notes)} notes\")\n","            batch_notes.clear()  # Clear batch to save memory\n","\n","    # final save\n","    if batch_notes:  # save remaining batch if not empty\n","        notes.extend(batch_notes)\n","        with open(notes_file, 'wb') as f:\n","            pickle.dump(notes, f)\n","        print(f\" Final save: Saved {len(notes)} notes\")\n","\n","    return notes"],"metadata":{"id":"6AphoeqnOvpQ","executionInfo":{"status":"ok","timestamp":1744367143756,"user_tz":-60,"elapsed":6,"user":{"displayName":"Fin Galletly","userId":"15278410467358031190"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["get_notes()"],"metadata":{"id":"Af3B7HauPkZw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_sequences(notes, n_vocab):\n","\tsequence_length = 128\n","\n","\t# get total vocab of component\n","\ttotal_comp = sorted(set(item for item in notes))\n","\n","\t # create a dictionary to map vocab to integers\n","\tnote_index_map = dict((note, number) for number, note in enumerate(total_comp))\n","\n","\tnetwork_input = []\n","\tnetwork_output = []\n","\n","\t# create input sequences and outputs\n","\tfor i in range(0, len(notes) - sequence_length, 1):\n","\t\tsequence_input = notes[i:i + sequence_length]\n","\t\tsequence_output = notes[i + sequence_length]\n","\t\tnetwork_input.append([note_index_map[char] for char in sequence_input])\n","\t\tnetwork_output.append(note_index_map[sequence_output])\n","\n","\tn_patterns = len(network_input)\n","\n","\t# reshape for LSTM compatibility\n","\tnetwork_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n","\n","\t# normalise input\n","\tnetwork_input = network_input / float(n_vocab)\n","\n","\tnetwork_output = to_categorical(network_output)\n","\n","\treturn (network_input, network_output)"],"metadata":{"id":"pv7XZ4FdPk9f","executionInfo":{"status":"ok","timestamp":1744367146685,"user_tz":-60,"elapsed":3,"user":{"displayName":"Fin Galletly","userId":"15278410467358031190"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def create_input_branch(input_data, name, lstm_units=256, dropout_rate=0.2):\n","    # layer for each feature branch\n","    input_layer = Input(shape=(input_data.shape[1], input_data.shape[2]), name=f\"input_{name}\")\n","    x = LSTM(\n","        lstm_units,\n","        return_sequences=True,\n","        name=f\"lstm_{name}\"\n","    )(input_layer)\n","    x = Dropout(dropout_rate, name=f\"dropout_{name}\")(x)\n","\n","    return input_layer, x"],"metadata":{"id":"_d6kmK-8Q_xV","executionInfo":{"status":"ok","timestamp":1744367148551,"user_tz":-60,"elapsed":2,"user":{"displayName":"Fin Galletly","userId":"15278410467358031190"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def create_output_branch(x, n_vocab, name, dense_units=128, dropout_rate=0.3):\n","    # output for each branch\n","    x = Dense(dense_units, activation='relu', name=f\"dense_{name}\")(x)\n","    x = BatchNormalization(name=f\"bn_{name}\")(x)\n","    x = Dropout(dropout_rate, name=f\"dropout_{name}\")(x)\n","    output = Dense(n_vocab, activation='softmax', name=name)(x)\n","\n","    return output"],"metadata":{"id":"ACszWdOTRXj9","executionInfo":{"status":"ok","timestamp":1744367149903,"user_tz":-60,"elapsed":1,"user":{"displayName":"Fin Galletly","userId":"15278410467358031190"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def create_network(network_input_notes, n_vocab_notes,\n","                  network_input_durations, n_vocab_durations,\n","                  network_input_offsets, n_vocab_offsets):\n","\n","    # Create input branches\n","    input_notes_layer, input_notes = create_input_branch(network_input_notes, \"notes\")\n","    input_durations_layer, input_durations = create_input_branch(network_input_durations, \"durations\")\n","    input_offsets_layer, input_offsets = create_input_branch(network_input_offsets, \"offsets\")\n","\n","    # Concatenate the three input branches\n","    combined = concatenate([input_notes, input_durations, input_offsets], name=\"combined_features\")\n","\n","    # Process combined features\n","    x = LSTM(512, return_sequences=True, name=\"lstm_combined_1\")(combined)\n","    x = Dropout(0.3, name=\"dropout_combined_1\")(x)\n","    x = LSTM(512, name=\"lstm_combined_2\")(x)\n","    x = BatchNormalization(name=\"bn_combined\")(x)\n","    x = Dropout(0.3, name=\"dropout_combined_2\")(x)\n","    x = Dense(256, activation='relu', name=\"dense_combined\")(x)\n","\n","    # Create output branches\n","    output_notes = create_output_branch(x, n_vocab_notes, \"Note\")\n","    output_durations = create_output_branch(x, n_vocab_durations, \"Duration\")\n","    output_offsets = create_output_branch(x, n_vocab_offsets, \"Offset\")\n","\n","    # Create and compile model\n","    model = Model(\n","        inputs=[input_notes_layer, input_durations_layer, input_offsets_layer],\n","        outputs=[output_notes, output_durations, output_offsets]\n","    )\n","\n","    model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","    # load weights to continue training\n","    # model.load_weights(weights_path)\n","\n","    return model"],"metadata":{"id":"xA695TdXRyQ1","executionInfo":{"status":"ok","timestamp":1744367259199,"user_tz":-60,"elapsed":7,"user":{"displayName":"Fin Galletly","userId":"15278410467358031190"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def train(model, network_input_notes, network_input_durations, network_input_offsets, network_output_notes, network_output_durations, network_output_offsets, epochs=200, batch_size=128):\n","\n","  filepath = Path(\"/content/drive/MyDrive/finalyearproject/weights/weight-{epoch:02d}-{loss:.4f}.keras\")\n","\n","  checkpoint = ModelCheckpoint(\n","\t\tfilepath,\n","\t\tmonitor='loss',\n","\t\tverbose=0,\n","\t\tsave_best_only=True,\n","\t\tmode='min'\n","\t)\n","\n","  # setup early stopping callback\n","  early_stopping = EarlyStopping(\n","      monitor='loss',\n","      patience=10,\n","      min_delta=0.0005,\n","      verbose=1,\n","      restore_best_weights=True  # restore model weights from the epoch with the best value\n","  )\n","\n","\n","  callbacks_list = [checkpoint, early_stopping]\n","\n","  model.fit([network_input_notes, network_input_durations, network_input_offsets], [network_output_notes, network_output_durations, network_output_offsets], epochs=200, batch_size=128, callbacks=callbacks_list, verbose=1)\n","\n","  # train the model\n","  history = model.fit(\n","      [network_input_notes, network_input_durations, network_input_offsets],\n","      [network_output_notes, network_output_durations, network_output_offsets],\n","      epochs=epochs,\n","      batch_size=batch_size,\n","      callbacks=callbacks_list,\n","      verbose=1\n","  )\n","\n","  return history\n"],"metadata":{"id":"rr1ayiymT0pd","executionInfo":{"status":"ok","timestamp":1744367256366,"user_tz":-60,"elapsed":42,"user":{"displayName":"Fin Galletly","userId":"15278410467358031190"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def train_network():\n","  notes_file = Path(\"/content/drive/MyDrive/finalyearproject/notes/notes7.pkl\")\n","  notes = []\n","  with open(notes_file, \"rb\") as file:\n","      notes = pickle.load(file)\n","\n","  # Get vocabs\n","  # Extract individual components for each note\n","  all_pitches = []\n","  all_durations = []\n","  all_offsets = []\n","  for item in notes:\n","      parts = item.split(\":\")\n","      all_pitches.append(parts[0])\n","      all_durations.append(parts[1])\n","      all_offsets.append(parts[2])\n","\n","\n","  pitches = sorted(set(all_pitches))\n","  durations = sorted(set(all_durations))\n","  offsets = sorted(set(all_offsets))\n","\n","  # prepare for model\n","  n_vocab_offsets = len(offsets)\n","  network_input_offsets, network_output_offsets = prepare_sequences(all_offsets, n_vocab_offsets)\n","\n","  n_vocab_notes = len(pitches)\n","  network_input_notes, network_output_notes = prepare_sequences(all_pitches, n_vocab_notes)\n","\n","  n_vocab_durations = len(durations)\n","  network_input_durations, network_output_durations = prepare_sequences(all_durations, n_vocab_durations)\n","\n","  # Create model\n","  model = create_network(\n","      network_input_notes, n_vocab_notes,\n","      network_input_durations, n_vocab_durations,\n","      network_input_offsets, n_vocab_offsets\n","  )\n","\n","  # Train model\n","  train(\n","      model,\n","      network_input_notes, network_input_durations, network_input_offsets,\n","      network_output_notes, network_output_durations, network_output_offsets\n","  )\n","\n","  # # Optional: Save final model\n","  # model.save(Path(\"/path\"))"],"metadata":{"id":"KMko60ZRV6zW","executionInfo":{"status":"ok","timestamp":1744375572101,"user_tz":-60,"elapsed":7,"user":{"displayName":"Fin Galletly","userId":"15278410467358031190"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["train_network()"],"metadata":{"id":"AHsUMD_R4rqQ"},"execution_count":null,"outputs":[]}]}